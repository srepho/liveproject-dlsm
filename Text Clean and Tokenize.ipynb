{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.fast.ai/text.transform.html\n",
    "https://github.com/fastai/fastai/blob/0b31610c6a836c56a337e2a34ee2d1510456d1c6/tests/test_text_transform.py#L19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "\n",
    "1. Load the dataset into a pandas dataframe.\n",
    "1. Use regular expressions to remove elements that are not words such as HTML tags, LaTeX expressions, URLs, digits, line returns, and so on.\n",
    "1. Remove missing values for texts\n",
    "1. Remove texts that are extremely large or too short to bring any information to the model. We want to keep paragraphs that contain at least a few words and remove the paragraphs that are composed of large numerical tables.\n",
    "1. Use a tokenizer to create a version of the original text that is a string of space-separated lowercase tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable\n",
    "\n",
    "* A .csv file that contains the original columns and a new column for the string of lowercase, space-separated tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "from urllib import request\n",
    "\n",
    "GZ_FILE   = 'stackexchange_812k.csv.gz'\n",
    "#DATA_FILE = 'stackexchange_812k.csv'\n",
    "DATA_URL  = 'https://liveproject-resources.s3.amazonaws.com/116/other/stackexchange_812k.csv.gz'\n",
    "if not path.exists(f'data/{GZ_FILE}'):\n",
    "    request.urlretrieve(DATA_URL, f'data/{GZ_FILE}')          \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load into Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "with gzip.open(f'data/{GZ_FILE}') as f:\n",
    "   df = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(txt):\n",
    "    txt = re.sub(r'<pre>.*?</pre>', r'', txt, flags=re.S)\n",
    "    txt = re.sub(r'<[^<]+?>', '', txt) #html tags\n",
    "    txt = re.sub(r'\\$[^$]+\\$', '', txt)  #latex\n",
    "    txt = re.sub(r'https?://[^\\s]*', '', txt) #remove URLs\n",
    "    txt = re.sub(r'\\s+', ' ', txt) #condense spaces \n",
    "    return txt\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_small = df[\"text\"].str.len() > 10\n",
    "df = df[remove_small]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id  parent_id  comment_id  \\\n",
       "0        1        NaN         NaN   \n",
       "1        2        NaN         NaN   \n",
       "2        3        NaN         NaN   \n",
       "3        4        NaN         NaN   \n",
       "4        6        NaN         NaN   \n",
       "\n",
       "                                                text category  \n",
       "0                      Eliciting priors from experts    title  \n",
       "1                                 What is normality?    title  \n",
       "2  What are some valuable Statistical Analysis op...    title  \n",
       "3  Assessing the significance of differences in d...    title  \n",
       "4  The Two Cultures: statistics vs. machine learn...    title  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Data\n",
    "\n",
    "You can skip this section unless you want to see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>211158</th>\n",
       "      <td>123567</td>\n",
       "      <td>123063.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In this my answer (a second and additional to ...</td>\n",
       "      <td>post</td>\n",
       "      <td>22088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155389</th>\n",
       "      <td>438347</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I would like to clean multiple time series of ...</td>\n",
       "      <td>post</td>\n",
       "      <td>20902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193171</th>\n",
       "      <td>316129</td>\n",
       "      <td>315502.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This answer aims to do four things: Review Ros...</td>\n",
       "      <td>post</td>\n",
       "      <td>18729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246925</th>\n",
       "      <td>247250</td>\n",
       "      <td>247094.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>If \"manually\" includes \"mechanical\" then you h...</td>\n",
       "      <td>post</td>\n",
       "      <td>16999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211091</th>\n",
       "      <td>123389</td>\n",
       "      <td>121852.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I am going to change the order of questions ab...</td>\n",
       "      <td>post</td>\n",
       "      <td>16892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        post_id  parent_id  comment_id  \\\n",
       "211158   123567   123063.0         NaN   \n",
       "155389   438347        NaN         NaN   \n",
       "193171   316129   315502.0         NaN   \n",
       "246925   247250   247094.0         NaN   \n",
       "211091   123389   121852.0         NaN   \n",
       "\n",
       "                                                     text category  length  \n",
       "211158  In this my answer (a second and additional to ...     post   22088  \n",
       "155389  I would like to clean multiple time series of ...     post   20902  \n",
       "193171  This answer aims to do four things: Review Ros...     post   18729  \n",
       "246925  If \"manually\" includes \"mechanical\" then you h...     post   16999  \n",
       "211091  I am going to change the order of questions ab...     post   16892  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#largest text fields\n",
    "df['length'] = df['text'].str.len()\n",
    "df.sort_values('length', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400901</th>\n",
       "      <td>181229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>344015.0</td>\n",
       "      <td>Maybe see:</td>\n",
       "      <td>comment</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705986</th>\n",
       "      <td>350492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>782409.0</td>\n",
       "      <td>Some dups:</td>\n",
       "      <td>comment</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261914</th>\n",
       "      <td>1889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2034.0</td>\n",
       "      <td>like so ;-)</td>\n",
       "      <td>comment</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487681</th>\n",
       "      <td>375856</td>\n",
       "      <td>NaN</td>\n",
       "      <td>706374.0</td>\n",
       "      <td>Please see</td>\n",
       "      <td>comment</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688593</th>\n",
       "      <td>333532</td>\n",
       "      <td>NaN</td>\n",
       "      <td>741790.0</td>\n",
       "      <td>[DataCamp](</td>\n",
       "      <td>comment</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        post_id  parent_id  comment_id         text category  length\n",
       "400901   181229        NaN    344015.0  Maybe see:   comment      11\n",
       "705986   350492        NaN    782409.0  Some dups:   comment      11\n",
       "261914     1889        NaN      2034.0  like so ;-)  comment      11\n",
       "487681   375856        NaN    706374.0  Please see   comment      11\n",
       "688593   333532        NaN    741790.0  [DataCamp](  comment      11"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shortest text fields\n",
    "df.sort_values('length', ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f6d25cc6f60>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length'].plot.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spacy tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df.apply(lambda row, nlp=nlp: ' '.join([t.text for t in nlp(row.text)]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>length</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>title</td>\n",
       "      <td>29</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>title</td>\n",
       "      <td>18</td>\n",
       "      <td>What is normality ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>title</td>\n",
       "      <td>65</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>title</td>\n",
       "      <td>58</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>title</td>\n",
       "      <td>50</td>\n",
       "      <td>The Two Cultures : statistics vs. machine lear...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id  parent_id  comment_id  \\\n",
       "0        1        NaN         NaN   \n",
       "1        2        NaN         NaN   \n",
       "2        3        NaN         NaN   \n",
       "3        4        NaN         NaN   \n",
       "4        6        NaN         NaN   \n",
       "\n",
       "                                                text category  length  \\\n",
       "0                      Eliciting priors from experts    title      29   \n",
       "1                                 What is normality?    title      18   \n",
       "2  What are some valuable Statistical Analysis op...    title      65   \n",
       "3  Assessing the significance of differences in d...    title      58   \n",
       "4  The Two Cultures: statistics vs. machine learn...    title      50   \n",
       "\n",
       "                                           tokenized  \n",
       "0                      Eliciting priors from experts  \n",
       "1                                What is normality ?  \n",
       "2  What are some valuable Statistical Analysis op...  \n",
       "3  Assessing the significance of differences in d...  \n",
       "4  The Two Cultures : statistics vs. machine lear...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fastai tokenization\n",
    "\n",
    "I haven't run this yet.  I have it hear just in case it might be a faster solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "texts  = df['text'].values\n",
    "tokens = tokenizer.process_all(texts) #faster to do it all at once?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized2'] = [' '.join(tt) for tt in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_FILE= 'stackexchange_tokenized.csv'\n",
    "df.to_csv(f'data/{OUT_FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am going to change the order of questions about . i \\'ve found textbooks and lecture notes frequently disagree , and would like a system to work through the choice that can safely be recommended as best practice , and especially a textbook or paper this can be cited to . xxmaj unfortunately , some discussions of this issue in books and so on rely on received wisdom . xxmaj sometimes that received wisdom is reasonable , sometimes it is less so ( at the least in the sense that it tends to focus on a smaller issue when a larger problem is ignored ) ; we should examine the justifications offered for the advice ( if any justification is offered at all ) with care . xxmaj most guides to choosing a t - test or non - parametric test focus on the normality issue . xxmaj that ’s true , but it ’s somewhat misguided for several reasons that i address in this answer . xxmaj if performing an \" unrelated samples \" or \" unpaired \" t - test , whether to use a xxmaj welch correction ? xxmaj this ( to use it unless you have reason to think variances should be equal ) is the advice of numerous references . i point to some in this answer . xxmaj some people use a hypothesis test for equality of variances , but here it would have low power . xxmaj generally i just eyeball whether the sample sds are \" reasonably \" close or not ( which is somewhat subjective , so there must be a more principled way of doing it ) but again , with low n it may well be that the population sds are rather further apart than the sample ones . xxmaj is it safer simply to always use the xxmaj welch correction for small samples , unless there is some good reason to believe population variances are equal ? xxmaj that ’s what the advice is . xxmaj the properties of the tests are affected by the choice based on the assumption test . xxmaj some references on this can be seen here and here , though there are more that say similar things . xxmaj the equal - variances issue has many similar characteristics to the normality issue – people want to test it , advice suggests conditioning choice of tests on the results of tests can adversely affect the results of both kinds of subsequent test – it ’s better simply not to assume what you ca n’t adequately justify ( by reasoning about the data , using information from other studies relating to the same variables and so on ) . xxmaj however , there are differences . xxmaj one is that – at least in terms of the distribution of the test statistic under the null hypothesis ( and hence , its level - robustness ) - non - normality is less important in large samples ( at least in respect of significance level , though power might still be an issue if you need to find small effects ) , while the effect of unequal variances under the equal variance assumption does n’t really go away with large sample size . xxmaj what principled method can be recommended for choosing which is the most appropriate test when the sample size is \" small \" ? xxmaj with hypothesis tests , what matters ( under some set of conditions ) is primarily two things : xxmaj what is the actual type i error rate ? xxmaj what is the power behaviour like ? xxmaj we also need to keep in mind that if we \\'re comparing two procedures , changing the first will change the second ( that is , if they ’re not conducted at the same actual significance level , you would expect that higher is associated with higher power ) . xxmaj with these small - sample issues in mind , is there a good - hopefully citable - checklist to work through when deciding between t and non - parametric tests ? i will consider a number of situations in which i ’ll make some recommendations , considering both the possibility of non - normality and unequal variances . xxmaj in every case , take mention of the t - test to imply the xxmaj welch - test : n medium - large xxmaj non - normal ( or unknown ) , likely to have near - equal variance : xxmaj if the distribution is heavy - tailed , you will generally be better with a xxmaj mann - xxmaj whitney , though if it ’s only slightly heavy , the t - test should do okay . xxmaj with light - tails the t - test may ( often ) be preferred . xxmaj permutation tests are a good option ( you can even do a permutation test using a t - statistic if you \\'re so inclined ) . xxmaj bootstrap tests are also suitable . xxmaj non - normal ( or unknown ) , unequal variance ( or variance relationship unknown ) : xxmaj if the distribution is heavy - tailed , you will generally be better with a xxmaj mann - xxmaj whitney - if inequality of variance is only related to inequality of mean - i.e. if xxup h0 is true the difference in spread should also be absent . glms are often a good option , especially if there ’s skewness and spread is related to the mean . a permutation test is another option , with a similar caveat as for the rank - based tests . xxmaj bootstrap tests are a good possibility here . xxmaj zimmerman and xxmaj zumbo ( 1993 ) suggest a xxmaj welch - t - test on the ranks which they say performs better that the xxmaj wilcoxon - xxmaj mann - xxmaj whitney in cases where the variances are unequal . n moderately small rank tests are reasonable defaults here if you expect non - normality ( again with the above caveat ) . xxmaj if you have external information about shape or variance , you might consider glms . xxmaj if you expect things not to be too far from normal , t - tests may be fine . n very small xxmaj because of the problem with getting suitable significance levels , neither permutation tests nor rank tests may be suitable , and at the smallest sizes , a t - test may be the best option ( there ’s some possibility of slightly robustifying it ) . xxmaj however , there ’s a good argument for using higher type i error rates with small samples ( otherwise you ’re letting type xxup ii error rates inflate while holding type i error rates constant ) . xxmaj also see de xxmaj winter ( 2013 ) . xxmaj the advice must be modified somewhat when the distributions are both strongly skewed and very discrete , such as xxmaj likert scale items where most of the observations are in one of the end categories . xxmaj then the xxmaj wilcoxon - xxmaj mann - xxmaj whitney is n’t necessarily a better choice than the t - test . xxmaj simulation can help guide choices further when you have some information about likely circumstances . i appreciate this is something of a perennial topic , but most questions concern the questioner \\'s particular data set , sometimes a more general discussion of power , and occasionally what to do if two tests disagree , but i would like a procedure to pick the correct test in the first place ! xxmaj the main problem is how hard it is to check the normality assumption in a small data set : xxmaj it is difficult to check normality in a small data set , and to some extent that \\'s an important issue , but i think there \\'s another issue of importance that we need to consider . a basic problem is that trying to assess normality as the basis of choosing between tests adversely impacts the properties of the tests you \\'re choosing between . xxmaj any formal test for normality would have low power so violations may well not be detected . ( xxmaj personally i would n\\'t test for this purpose , and i \\'m clearly not alone , but i \\'ve found this little use when clients demand a normality test be performed because that \\'s what their textbook or old lecture notes or some website they found once declare should be done . xxmaj this is one point where a weightier looking citation would be welcome . ) xxmaj here ’s an example of a reference ( there are others ) which is unequivocal ( xxmaj fay and xxmaj proschan , 2010 ) : xxmaj the choice between t- and xxup wmw drs should not be based on a test of normality . xxmaj they are similarly unequivocal about not testing for equality of variance . xxmaj to make matters worse , it is unsafe to use the xxmaj central xxmaj limit xxmaj theorem as a safety net : for small n we ca n\\'t rely on the convenient asymptotic normality of the test statistic and t distribution . xxmaj nor even in large samples -- asymptotic normality of the numerator does n’t imply that the t - statistic will have a t - distribution . xxmaj however , that may not matter so much , since you should still have asymptotic normality ( e.g. xxup clt for the numerator , and xxmaj slutsky ’s theorem suggest that eventually the t - statistic should begin to look normal , if the conditions for both hold . ) xxmaj one principled response to this is \" safety first \" : as there \\'s no way to reliably verify the normality assumption on a small sample , run an equivalent non - parametric test instead . xxmaj that ’s actually the advice that the references i mention ( or link to mentions of ) give . xxmaj another approach i \\'ve seen but feel less comfortable with , is to perform a visual check and proceed with a t - test if nothing untowards is observed ( \" no reason to reject normality \" , ignoring the low power of this check ) . xxmaj my personal inclination is to consider whether there are any grounds for assuming normality , theoretical ( e.g. variable is sum of several random components and xxup clt applies ) or empirical ( e.g. previous studies with larger n suggest variable is normal ) . xxmaj both those are good arguments , especially when backed up with the fact that the t - test is reasonably robust against moderate deviations from normality . ( xxmaj one should keep in mind , however , that \" moderate deviations \" is a tricky phrase ; certain kinds of deviations from normality may impact the power performace of the t - test quite a bit even though those deviations are visually very small - the t - test is less robust to some deviations than others . xxmaj we should keep this in mind whenever we \\'re discussing small deviations from normality . ) xxmaj beware , however , the phrasing \" suggest the variable is normal \" . xxmaj being reasonably consistent with normality is not the same thing as normality . xxmaj we can often reject actual normality with no need even to see the data – for example , if the data can not be negative , the distribution can not be normal . xxmaj fortunately , what matters is closer to what we might actually have from previous studies or reasoning about how the data are composed , which is that the deviations from normality should be small . xxmaj if so , i would use a t - test if data passed visual inspection , and otherwise stick to non - parametrics . xxmaj but any theoretical or empirical grounds usually only justify assuming approximate normality , and on low degrees of freedom it \\'s hard to judge how near normal it needs to be to avoid invalidating a t - test . xxmaj well , that ’s something we can assess the impact of fairly readily ( such as via simulations , as i mentioned earlier ) . xxmaj from what i \\'ve seen , skewness seems to matter more than heavy tails ( but on the other hand i have seen some claims of the opposite - though i do n\\'t know what that \\'s based on ) . xxmaj for people who see the choice of methods as a trade - off between power and robustness , claims about the asymptotic efficiency of the non - parametric methods are unhelpful . xxmaj for instance , the rule of thumb that \" xxmaj wilcoxon tests have about 95 % of the power of a t - test if the data really are normal , and are often far more powerful if the data is not , so just use a xxmaj wilcoxon \" is sometimes heard , but if the 95 % only applies to large n , this is flawed reasoning for smaller samples . xxmaj but we can check small - sample power quite easily ! xxmaj it ’s easy enough to simulate to obtain power curves as here . ( xxmaj again , also see de xxmaj winter ( 2013 ) ) . xxmaj having done such simulations under a variety of circumstances , both for the two - sample and one - sample / paired - difference cases , the small sample efficiency at the normal in both cases seems to be a little lower than the asymptotic efficiency , but the efficiency of the signed rank and xxmaj wilcoxon - xxmaj mann - xxmaj whitney tests is still very high even at very small sample sizes . xxmaj at least that \\'s if the tests are done at the same actual significance level ; you ca n\\'t do a 5 % test with very small samples ( and least not without randomized tests for example ) , but if you \\'re prepared to perhaps do ( say ) a 5.5 % or a 3.2 % test instead , then the rank tests hold up very well indeed compared with a t - test at that significance level . xxmaj small samples may make it very difficult , or impossible , to assess whether a transformation is appropriate for the data since it \\'s hard to tell whether the transformed data belong to a ( sufficiently ) normal distribution . xxmaj so if a xxup qq plot reveals very positively skewed data , which look more reasonable after taking logs , is it safe to use a t - test on the logged data ? xxmaj on larger samples this would be very tempting , but with small n i \\'d probably hold off unless there had been grounds to expect a log - normal distribution in the first place . xxmaj there ’s another alternative : make a different parametric assumption . xxmaj for example , if there ’s skewed data , one might , for example , in some situations reasonably consider a gamma distribution , or some other skewed family as a better approximation - in moderately large samples , we might just use a xxup glm , but in very small samples it may be necessary to look to a small - sample test - in many cases simulation can be useful . xxmaj alternative 2 : robustify the t - test ( but taking care about the choice of robust procedure so as not to heavily discretize the resulting distribution of the test statistic ) - this has some advantages over a very - small - sample nonparametric procedure such as the ability to consider tests with low type i error rate . xxmaj here i \\'m thinking along the lines of using say m - estimators of location ( and related estimators of scale ) in the t - statistic to smoothly robustify against deviations from normality . xxmaj something akin to the xxmaj welch , like : $ \\\\ stackrel { \\\\ sim}{s}_p^2= \\\\ frac { \\\\ stackrel { \\\\ sim}{s}_x^2}{n_x}+ \\\\ frac { \\\\ stackrel { \\\\ sim}{s}_y^2}{n_y } \\\\ stackrel { \\\\ sim}{x } \\\\ stackrel { \\\\ sim}{s}_x \\\\ psin$ is very small indeed ( around 3 - 5 , in each sample , say ) , so even m - estimation potentially has its issues . xxmaj you could , for example , use simulation at the normal to get p - values ( if sample sizes are very small , i \\'d suggest that over bootstrapping - if sample sizes are n\\'t so small , a carefully - implemented bootstrap may do quite well , but then we might as well go back to xxmaj wilcoxon - xxmaj mann - xxmaj whitney ) . xxmaj there \\'s be a scaling factor as well as a d.f . adjustment to get to what i \\'d imagine would then be a reasonable t - approximation . xxmaj this means we should get the kind of properties we seek very close to the normal , and should have reasonable robustness in the broad vicinity of the normal . xxmaj there are a number of issues that come up that would be outside the scope of the present question , but i think in very small samples the benefits should outweigh the costs and the extra effort required . [ i have n\\'t read the literature on this stuff for a very long time , so i do n\\'t have suitable references to offer on that score . ] xxmaj of course if you did n\\'t expect the distribution to be somewhat normal - like , but rather similar to some other distribution , you could undertake a suitable robustification of a different parametric test . xxmaj what if you want to check assumptions for the non - parametrics ? xxmaj some sources recommend verifying a symmetric distribution before applying a xxmaj wilcoxon test , which brings up similar problems to checking normality . xxmaj indeed . i assume you mean the signed rank test*. xxmaj in the case of using it on paired data , if you are prepared to assume that the two distributions are the same shape apart from location shift you are safe , since the differences should then be symmetric . xxmaj actually , we do n\\'t even need that much ; for the test to work you need symmetry under the null ; it \\'s not required under the alternative ( e.g. consider a paired situation with identically - shaped right skewed continuous distributions on the positive half - line , where the scales differ under the alternative but not under the null ; the signed rank test should work essentially as expected in that case ) . xxmaj the interpretation of the test is easier if the alternative is a location shift though . * ( xxmaj wilcoxon ’s name is associated with both the one and two sample rank tests – signed rank and rank sum ; with their u test , xxmaj mann and xxmaj whitney generalized the situation studied by xxmaj wilcoxon , and introduced important new ideas for evaluating the null distribution , but the priority between the two sets of authors on the xxmaj wilcoxon - xxmaj mann - xxmaj whitney is clearly xxmaj wilcoxon ’s -- so at least if we only consider xxmaj wilcoxon vs mann&&whitney , xxmaj wilcoxon goes first in my book . xxmaj however , it seems xxmaj stigler \\'s xxmaj law beats me yet again , and xxmaj wilcoxon should perhaps share some of that priority with a number of earlier contributors , and ( besides xxmaj mann and xxmaj whitney ) should share credit with several discoverers of an equivalent test.[4][5 ] ) xxmaj references [ 1 ] : xxmaj zimmerman xxup dw and xxmaj zumbo xxup bn , ( 1993 ) , xxmaj rank transformations and the power of the xxmaj student t - test and xxmaj welch t′-test for non - normal populations , xxmaj canadian xxmaj journal xxmaj experimental xxmaj psychology , 47 : 523–39 . [ 2 ] : xxup j.c.f. de xxmaj winter ( 2013 ) , \" xxmaj using the xxmaj student ’s t - test with extremely small sample sizes , \" xxmaj practical xxmaj assessment , xxmaj research and xxmaj evaluation , 18:10 , xxmaj august , xxup issn 1531 - 7714 [ 3 ] : xxmaj michael xxup p. xxmaj fay and xxmaj michael xxup a. xxmaj proschan ( 2010 ) , \" xxmaj wilcoxon - xxmaj mann - xxmaj whitney or t - test ? xxmaj on assumptions for hypothesis tests and multiple interpretations of decision rules , \" xxmaj stat xxmaj surv ; 4 : 1–39 . [ 4 ] : xxmaj berry , xxup k.j. , xxmaj mielke , xxup p.w. and xxmaj johnston , xxup j.e. ( 2012 ) , \" xxmaj the xxmaj two - sample xxmaj rank - sum xxmaj test : xxmaj early xxmaj development , \" xxmaj electronic xxmaj journal for xxmaj history of xxmaj probability and xxmaj statistics , xxmaj vol.8 , xxmaj december pdf [ 5 ] : xxmaj kruskal , xxup w. xxup h. ( 1957 ) , \" xxmaj historical notes on the xxmaj wilcoxon unpaired two - sample test , \" xxmaj journal of the xxmaj american xxmaj statistical xxmaj association , 52 , 356–360 .'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#easy to combine spacy and fastai. \n",
    "\n",
    "#tokenizer = Tokenizer()\n",
    "tok = SpacyTokenizer('en')\n",
    "' '.join(tokenizer.process_text(df.loc[211091].text, tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['one two three four', 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.', \"I'm suddenly SHOUTING FOR NO REASON\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['one', 'two', 'three', 'four'],\n",
       " ['xxmaj',\n",
       "  'lorem',\n",
       "  'ipsum',\n",
       "  'dolor',\n",
       "  'sit',\n",
       "  'amet,',\n",
       "  'consectetur',\n",
       "  'adipiscing',\n",
       "  'elit,',\n",
       "  'sed',\n",
       "  'do',\n",
       "  'eiusmod',\n",
       "  'tempor',\n",
       "  'incididunt',\n",
       "  'ut',\n",
       "  'labore',\n",
       "  'et',\n",
       "  'dolore',\n",
       "  'magna',\n",
       "  'aliqua.'],\n",
       " ['xxmaj',\n",
       "  \"i'm\",\n",
       "  'suddenly',\n",
       "  'xxup',\n",
       "  'shouting',\n",
       "  'xxup',\n",
       "  'for',\n",
       "  'xxup',\n",
       "  'no',\n",
       "  'xxup',\n",
       "  'reason']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.process_all(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the dataset into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "with gzip.open(f'data/{GZ_FILE}') as f:\n",
    "   df = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id  parent_id  comment_id  \\\n",
       "0        1        NaN         NaN   \n",
       "1        2        NaN         NaN   \n",
       "2        3        NaN         NaN   \n",
       "3        4        NaN         NaN   \n",
       "4        6        NaN         NaN   \n",
       "\n",
       "                                                text category  \n",
       "0                      Eliciting priors from experts    title  \n",
       "1                                 What is normality?    title  \n",
       "2  What are some valuable Statistical Analysis op...    title  \n",
       "3  Assessing the significance of differences in d...    title  \n",
       "4  The Two Cultures: statistics vs. machine learn...    title  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use regular expressions to remove elements that are not words such as HTML tags, LaTeX expressions, URLs, digits, line returns, and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<b>hello<', '/', 'b>', '1234']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what will fastai do here?\n",
    "html = fix_html(\"<b>hello</b> 1234\") #didn't really fix it. \n",
    "tokenizer.process_all([html])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text begin text end'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#simple RE approach\n",
    "import re\n",
    "\n",
    "pre = \"text begin<pre>code inside!\\n\\n</pre> text end\"\n",
    "latex = r'hello $y = mx + b$ is my equation'\n",
    "url = \"my favorite website is https://www.stylemepretty.com. Love it\"\n",
    "spaces = \"this is no good.\\r\\n no good.    bad\\r\\r\\r\\n\\n boy.\"\n",
    "\n",
    "text5 = re.sub(r'<pre>.*?</pre>', r'', pre, flags=re.S)\n",
    "text = re.sub(r'<[^<]+?>', '', html)\n",
    "txt2 = re.sub(r'\\$[^$]+\\$', '', latex)\n",
    "txt3 = re.sub(r'https?://[^\\s]*', '', url) #remove URLs\n",
    "txt4 = re.sub(r'\\s+', ' ', spaces) #condense spaces\n",
    "\n",
    "#remove punctuation\n",
    "\n",
    "text5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'favorite', 'website', 'is', 'https://www.stylemepretty.com', '-', 'boom', '.', ':)']\n"
     ]
    }
   ],
   "source": [
    "#messing with spacy\n",
    "# https://spacy.io/usage/linguistic-features#native-tokenizers\n",
    "import spacy\n",
    "import re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "special_cases = {\":)\": [{\"ORTH\": \":)\"}]}\n",
    "prefix_re = re.compile(r'''^[[(\"']''')\n",
    "suffix_re = re.compile(r'''[])\"']$''')\n",
    "infix_re = re.compile(r'''[-~]''')\n",
    "simple_url_re = re.compile(r'''^https?://''')\n",
    "\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp = English()\n",
    "tok = Tokenizer(nlp.vocab, rules=special_cases,\n",
    "                prefix_search=prefix_re.search,\n",
    "                suffix_search=suffix_re.search,\n",
    "                infix_finditer=infix_re.finditer,\n",
    "                #token_match=simple_url_re.match\n",
    "               )\n",
    "\n",
    "#nlp.tokenizer = tok\n",
    "doc = nlp(\"my favorite website is https://www.stylemepretty.com - boom. :)\") #suffix_re matches to 'boom.'\n",
    "print([t.text for t in doc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove missing values for texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [post_id, parent_id, comment_id, text, category]\n",
       "Index: []"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"text\"].isnull()] #no text is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id  parent_id  comment_id  \\\n",
       "0        1        NaN         NaN   \n",
       "1        2        NaN         NaN   \n",
       "2        3        NaN         NaN   \n",
       "3        4        NaN         NaN   \n",
       "4        6        NaN         NaN   \n",
       "\n",
       "                                                text category  \n",
       "0                      Eliciting priors from experts    title  \n",
       "1                                 What is normality?    title  \n",
       "2  What are some valuable Statistical Analysis op...    title  \n",
       "3  Assessing the significance of differences in d...    title  \n",
       "4  The Two Cultures: statistics vs. machine learn...    title  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minsize = df[\"text\"].str.len() > 10\n",
    "maxsize = df[\"text\"].str.len() < 300\n",
    "df[maxsize & minsize].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the paragraphs that are composed of large numerical tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91755</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;I have two groups of data.  Each with a dif...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91756</th>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;The R-project&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\"http://www...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91757</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Last year, I read a blog post from &lt;a href=...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91758</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;I've been working on a new method for analy...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91762</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Is there a good, modern treatment covering ...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       post_id  parent_id  comment_id  \\\n",
       "91755        4        NaN         NaN   \n",
       "91756        5        3.0         NaN   \n",
       "91757        6        NaN         NaN   \n",
       "91758        7        NaN         NaN   \n",
       "91762       11        NaN         NaN   \n",
       "\n",
       "                                                    text category  \n",
       "91755  <p>I have two groups of data.  Each with a dif...     post  \n",
       "91756  <p>The R-project</p>\\n\\n<p><a href=\"http://www...     post  \n",
       "91757  <p>Last year, I read a blog post from <a href=...     post  \n",
       "91758  <p>I've been working on a new method for analy...     post  \n",
       "91762  <p>Is there a good, modern treatment covering ...     post  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "very_big = df[\"text\"].str.len() > 300\n",
    "df[very_big].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<p>I have two groups of data.  Each with a different distribution of multiple variables.  I'm trying to determine if these two groups' distributions are different in a statistically significant way.  I have the data in both raw form and binned up in easier to deal with discrete categories with frequency counts in each.  </p>\\n\\n<p>What tests/procedures/methods should I use to determine whether or not these two groups are significantly different and how do I do that in SAS or R (or Orange)?</p>\\n\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[91755].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = df[\"text\"].str.contains(r'\\d{8,}', regex=True)\n",
    "# contains can do regular expressions. Try that\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.contains.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91757</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Last year, I read a blog post from &lt;a href=...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91796</th>\n",
       "      <td>71</td>\n",
       "      <td>58.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;It's an algorithm for training feedforward ...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91826</th>\n",
       "      <td>126</td>\n",
       "      <td>125.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;My favorite is &lt;a href=\"http://www.amazon.c...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91839</th>\n",
       "      <td>151</td>\n",
       "      <td>118.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;If the goal of the standard deviation is to...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91883</th>\n",
       "      <td>231</td>\n",
       "      <td>223.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;This is one I've used successfully:&lt;/p&gt;\\n\\n...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       post_id  parent_id  comment_id  \\\n",
       "91757        6        NaN         NaN   \n",
       "91796       71       58.0         NaN   \n",
       "91826      126      125.0         NaN   \n",
       "91839      151      118.0         NaN   \n",
       "91883      231      223.0         NaN   \n",
       "\n",
       "                                                    text category  \n",
       "91757  <p>Last year, I read a blog post from <a href=...     post  \n",
       "91796  <p>It's an algorithm for training feedforward ...     post  \n",
       "91826  <p>My favorite is <a href=\"http://www.amazon.c...     post  \n",
       "91839  <p>If the goal of the standard deviation is to...     post  \n",
       "91883  <p>This is one I've used successfully:</p>\\n\\n...     post  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[bar].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>My favorite is <a href=\"http://www.amazon.com/exec/obidos/ISBN=158488388X/\">\"Bayesian Data Analysis\"</a> by Gelman, et al.</p>\\n'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[91826].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99414</th>\n",
       "      <td>13845</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;I’m working on a trading system and need to...</td>\n",
       "      <td>post</td>\n",
       "      <td>38847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124323</th>\n",
       "      <td>67228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;How do I calculate the uncertainties in lin...</td>\n",
       "      <td>post</td>\n",
       "      <td>35321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235896</th>\n",
       "      <td>215962</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;I am trying to determine the habitat of a s...</td>\n",
       "      <td>post</td>\n",
       "      <td>33306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249773</th>\n",
       "      <td>254466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;I have a time series Y, for one year and me...</td>\n",
       "      <td>post</td>\n",
       "      <td>29837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183579</th>\n",
       "      <td>286236</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;I have a fitted mixed-effects model with a ...</td>\n",
       "      <td>post</td>\n",
       "      <td>29457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        post_id  parent_id  comment_id  \\\n",
       "99414     13845        NaN         NaN   \n",
       "124323    67228        NaN         NaN   \n",
       "235896   215962        NaN         NaN   \n",
       "249773   254466        NaN         NaN   \n",
       "183579   286236        NaN         NaN   \n",
       "\n",
       "                                                     text category  length  \n",
       "99414   <p>I’m working on a trading system and need to...     post   38847  \n",
       "124323  <p>How do I calculate the uncertainties in lin...     post   35321  \n",
       "235896  <p>I am trying to determine the habitat of a s...     post   33306  \n",
       "249773  <p>I have a time series Y, for one year and me...     post   29837  \n",
       "183579  <p>I have a fitted mixed-effects model with a ...     post   29457  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#largest text fields\n",
    "df['length'] = df['text'].str.len()\n",
    "df.sort_values('length', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>I’m working on a trading system and need to apply some statistics on the results. Unfortunately I forgot all about statistics after I left university over a decade ago and now I really have no clue how I must calculate what I need. Hopefully someone can help me out.</p>\\n\\n<p>Out of the trading application (currently in test mode), I get profit / loss (PL) per trade and per day.</p>\\n\\n<p>Let’s say I have the day-to-day PL (an accumulation will give the total PL over the given period) of 5 years back testing (about 1250 points), what is the best way of “predicting” what the total profit might be in the next 6 months (125 points ahead) and the next year (250 points ahead)?</p>\\n\\n<p>Of course not every trade is profitable. So I have some trades with losses and (hopefully) more trades with profit.</p>\\n\\n<p>What is the best way of calculating what the profit per day (with a certain reliability) will be when you only take the winning trades into account, what lose will be when you only look at the losing trades and what the PL will be when you take both winning and losing trade combined.</p>\\n\\n<p>How can I calculate the change that over an X period of time, the strategy will, with a certain percentage of reliability) make money?</p>\\n\\n<p>Furthermore, I will of course not trade just one strategy, but multiple. Based on back testing, how can I calculate the likelihood that the portfolio I will be trading will make money in x period and how can I calculate the bandwidth for the expected profit for the pool? </p>\\n\\n<p>Hope you can help me out which methods to use (I’ll be programming it in C# by the way).</p>\\n\\n<hr>\\n\\n<p><strong>Example of day-to-day PL (rounded rounded to 0 decimals for the layout):</strong></p>\\n\\n<pre>\\nDBDate  PeriodToPeriodPL        Accumulation\\n20060720    0       \\n20060721    -1741       -1741\\n20060724    -716        -2458\\n20060725    -821        -3279\\n20060726    -149        -3427\\n20060727    -3517       -6945\\n20060728    2253        -4692\\n20060731    97      -4594\\n20060801    54      -4541\\n20060802    329     -4212\\n20060803    726     -3486\\n20060804    569     -2917\\n20060807    1359        -1558\\n20060808    -682        -2240\\n20060809    -635        -2874\\n20060810    -277        -3152\\n20060811    -255        -3406\\n20060814    -542        -3948\\n20060815    -1320       -5268\\n20060816    932     -4336\\n20060817    2553        -1783\\n20060818    -155        -1938\\n20060821    -367        -2305\\n20060822    669     -1637\\n20060823    433     -1204\\n20060824    -734        -1938\\n20060825    361     -1577\\n20060828    -479        -2056\\n20060829    -363        -2419\\n20060830    -1268       -3686\\n20060831    -424        -4111\\n20060901    779     -3331\\n20060904    1857        -1475\\n20060905    -356        -1831\\n20060906    -163        -1994\\n20060907    -458        -2452\\n20060908    1130        -1322\\n20060911    -254        -1576\\n20060912    -731        -2308\\n20060913    -442        -2749\\n20060914    986     -1763\\n20060915    -497        -2260\\n20060918    1236        -1024\\n20060919    -842        -1866\\n20060920    80      -1786\\n20060921    -2628       -4413\\n20060922    185     -4228\\n20060925    -111        -4339\\n20060926    377     -3963\\n20060927    -1806       -5769\\n20060928    881     -4888\\n20060929    -800        -5688\\n20061002    2010        -3677\\n20061003    794     -2884\\n20061004    1063        -1821\\n20061005    2214        393\\n20061006    1110        1503\\n20061009    1401        2903\\n20061010    -65     2838\\n20061011    -77     2761\\n20061012    -787        1974\\n20061013    -1512       462\\n20061016    300     762\\n20061017    1058        1820\\n20061018    1625        3445\\n20061019    773     4218\\n20061020    1854        6072\\n20061023    492     6564\\n20061024    -1063       5501\\n20061025    1230        6731\\n20061026    148     6878\\n20061027    11      6889\\n20061030    -1189       5700\\n20061031    -47     5652\\n20061101    -484        5168\\n20061102    -152        5017\\n20061103    -998        4019\\n20061106    -591        3428\\n20061107    -970        2458\\n20061108    378     2836\\n20061109    1217        4052\\n20061110    745     4797\\n20061113    201     4998\\n20061114    -454        4545\\n20061115    149     4693\\n20061116    134     4828\\n20061117    -677        4151\\n20061120    1404        5555\\n20061121    864     6418\\n20061122    271     6689\\n20061123    1503        8192\\n20061124    4577        12769\\n20061127    2245        15014\\n20061128    -2902       12112\\n20061129    2002        14114\\n20061130    1108        15222\\n20061201    1256        16479\\n20061204    2236        18715\\n20061205    -1496       17219\\n20061206    842     18061\\n20061207    -1639       16422\\n20061208    650     17072\\n20061211    248     17320\\n20061212    -218        17102\\n20061213    -873        16228\\n20061214    -364        15865\\n20061215    -27     15838\\n20061218    -543        15295\\n20061219    -974        14321\\n20061220    2152        16473\\n20061221    3382        19855\\n20061222    -22     19832\\n20061227    -44     19789\\n20061228    1497        21286\\n20061229    979     22265\\n20070102    -1136       21129\\n20070103    -620        20509\\n20070104    -2040       18469\\n20070105    -3541       14927\\n20070108    -337        14591\\n20070109    3120        17711\\n20070110    -1904       15806\\n20070111    -778        15028\\n20070112    514     15542\\n20070115    141     15683\\n20070116    828     16511\\n20070117    2184        18694\\n20070118    769     19463\\n20070119    -391        19072\\n20070122    -1349       17723\\n20070123    -231        17492\\n20070124    1493        18985\\n20070125    -7452       11533\\n20070126    -1504       10029\\n20070129    496     10526\\n20070130    4924        15450\\n20070131    -482        14968\\n20070201    -683        14285\\n20070202    -2375       11910\\n20070205    509     12420\\n20070206    3099        15519\\n20070207    -765        14753\\n20070208    859     15613\\n20070209    -162        15451\\n20070212    -61     15390\\n20070213    -1330       14060\\n20070214    884     14944\\n20070215    -524        14420\\n20070216    1504        15924\\n20070219    966     16890\\n20070220    577     17467\\n20070221    3182        20649\\n20070222    2579        23228\\n20070223    -2234       20994\\n20070226    -1637       19357\\n20070227    683     20040\\n20070228    1572        21612\\n20070301    -83     21528\\n20070302    371     21899\\n20070305    1089        22988\\n20070306    -470        22518\\n20070307    157     22675\\n20070308    -839        21837\\n20070309    209     22046\\n20070312    1658        23705\\n20070313    -970        22734\\n20070314    -1319       21415\\n20070315    1298        22713\\n20070316    204     22918\\n20070319    -894        22024\\n20070320    -1189       20835\\n20070321    -231        20604\\n20070322    -1939       18666\\n20070323    189     18854\\n20070326    -364        18490\\n20070327    616     19106\\n20070328    -2183       16922\\n20070329    570     17492\\n20070330    897     18389\\n20070402    501     18889\\n20070403    1067        19956\\n20070404    -243        19714\\n20070405    -2119       17595\\n20070410    2158        19753\\n20070411    -753        19000\\n20070412    -648        18353\\n20070413    1046        19399\\n20070416    1773        21172\\n20070417    -312        20859\\n20070418    203     21062\\n20070419    3498        24560\\n20070420    5839        30399\\n20070423    1620        32019\\n20070424    -3004       29015\\n20070425    -1182       27833\\n20070426    -1897       25936\\n20070427    -1849       24087\\n20070430    0       24087\\n20070502    0       24087\\n20070503    0       24087\\n20070504    0       24087\\n20070507    1515        25602\\n20070508    899     26501\\n20070509    120     26620\\n20070510    -743        25878\\n20070511    6129        32007\\n20070514    -618        31389\\n20070515    -321        31068\\n20070516    -2096       28972\\n20070517    9       28981\\n20070518    3362        32343\\n20070521    806     33149\\n20070522    824     33973\\n20070523    41      34014\\n20070524    4741        38755\\n20070525    1337        40092\\n20070529    -258        39834\\n20070530    -1037       38797\\n20070531    -2438       36359\\n20070601    -227        36132\\n20070604    -1223       34909\\n20070605    -309        34601\\n20070606    465     35065\\n20070607    -1273       33792\\n20070608    1515        35308\\n20070611    -1985       33323\\n20070612    0       33323\\n20070613    977     34299\\n20070614    3523        37823\\n20070615    2774        40597\\n20070618    -1459       39138\\n20070619    1491        40628\\n20070620    2928        43556\\n20070621    -1287       42269\\n20070622    436     42706\\n20070625    151     42857\\n20070626    509     43366\\n20070627    -626        42741\\n20070628    2392        45133\\n20070629    -1406       43727\\n20070702    510     44236\\n20070703    2164        46401\\n20070704    1362        47763\\n20070705    123     47886\\n20070706    -1548       46338\\n20070709    -994        45344\\n20070710    1279        46623\\n20070711    993     47616\\n20070712    -274        47341\\n20070713    775     48116\\n20070716    1083        49200\\n20070717    1581        50780\\n20070718    272     51052\\n20070719    -1402       49650\\n20070720    1273        50924\\n20070723    -601        50322\\n20070724    -423        49899\\n20070725    4208        54107\\n20070726    5212        59320\\n20070727    -1074       58246\\n20070730    -1086       57160\\n20070731    2303        59463\\n20070801    -851        58613\\n20070802    3749        62362\\n20070803    -573        61788\\n20070806    161     61950\\n20070807    -737        61213\\n20070808    447     61660\\n20070809    85      61745\\n20070810    -1190       60555\\n20070813    -2268       58287\\n20070814    -1279       57008\\n20070815    1787        58796\\n20070816    2640        61435\\n20070817    200     61636\\n20070820    -2181       59454\\n20070821    228     59683\\n20070822    -2426       57256\\n20070823    469     57725\\n20070824    894     58619\\n20070827    -719        57900\\n20070828    -2274       55625\\n20070829    -609        55016\\n20070830    -354        54662\\n20070831    -1308       53354\\n20070903    -754        52600\\n20070904    1138        53738\\n20070905    580     54318\\n20070906    700     55018\\n20070907    -3132       51887\\n20070910    1395        53282\\n20070911    -852        52430\\n20070912    1468        53898\\n20070913    463     54361\\n20070914    2815        57176\\n20070917    2957        60133\\n20070918    -612        59521\\n20070919    -4888       54633\\n20070920    384     55017\\n20070921    3331        58348\\n20070924    -626        57722\\n20070925    -2263       55459\\n20070926    -2498       52961\\n20070927    -2361       50600\\n20070928    286     50887\\n20071001    -1173       49713\\n20071002    1842        51555\\n20071003    582     52138\\n20071004    -400        51737\\n20071005    -374        51363\\n20071008    -2136       49227\\n20071009    188     49414\\n20071010    -214        49200\\n20071011    11      49211\\n20071012    439     49650\\n20071015    3112        52762\\n20071016    4980        57742\\n20071017    484     58225\\n20071018    2472        60697\\n20071019    -802        59896\\n20071022    1324        61220\\n20071023    -816        60404\\n20071024    3213        63617\\n20071025    92      63709\\n20071026    -413        63296\\n20071029    -2371       60925\\n20071030    -550        60375\\n20071031    1879        62254\\n20071101    -389        61865\\n20071102    321     62186\\n20071105    1033        63218\\n20071106    1505        64724\\n20071107    -1148       63576\\n20071108    9273        72849\\n20071109    -1341       71508\\n20071112    -86     71422\\n20071113    -1523       69899\\n20071114    5691        75590\\n20071115    -825        74765\\n20071116    -3407       71358\\n20071119    -2476       68882\\n20071120    923     69805\\n20071121    1226        71031\\n20071122    507     71539\\n20071123    -3045       68494\\n20071126    316     68810\\n20071127    -434        68376\\n20071128    978     69354\\n20071129    1120        70474\\n20071130    1842        72315\\n20071203    -1072       71243\\n20071204    -2216       69027\\n20071205    -1354       67673\\n20071206    1242        68915\\n20071207    -691        68224\\n20071210    -2518       65706\\n20071211    1337        67044\\n20071212    -581        66462\\n20071213    -1408       65054\\n20071214    145     65198\\n20071217    -3034       62165\\n20071218    -1477       60687\\n20071219    -2574       58113\\n20071220    0       58113\\n20071221    3305        61419\\n20071227    -1763       59655\\n20071228    114     59770\\n20080102    -731        59039\\n20080103    4131        63170\\n20080104    2157        65327\\n20080107    4497        69825\\n20080108    -3348       66477\\n20080109    5641        72118\\n20080110    827     72945\\n20080111    -2459       70486\\n20080114    -4188       66297\\n20080115    -2038       64260\\n20080116    -1244       63016\\n20080117    0       63016\\n20080118    0       63016\\n20080121    1429        64445\\n20080122    -6762       57683\\n20080123    1834        59517\\n20080124    -2579       56938\\n20080125    -1874       55064\\n20080128    -1050       54013\\n20080129    -3025       50989\\n20080130    0       50989\\n20080131    657     51646\\n20080201    3606        55252\\n20080204    550     55802\\n20080205    -4845       50957\\n20080206    659     51617\\n20080207    2697        54314\\n20080208    -746        53568\\n20080211    838     54406\\n20080212    -448        53958\\n20080213    -121        53837\\n20080214    -1257       52581\\n20080215    -324        52257\\n20080218    502     52759\\n20080219    -171        52587\\n20080220    1108        53695\\n20080221    -203        53493\\n20080222    5385        58878\\n20080225    772     59650\\n20080226    2802        62452\\n20080227    940     63392\\n20080228    -3395       59998\\n20080229    -3194       56804\\n20080303    63      56866\\n20080304    458     57324\\n20080305    -2545       54780\\n20080306    3370        58149\\n20080307    712     58861\\n20080310    3039        61900\\n20080311    -658        61241\\n20080312    -1338       59904\\n20080313    1720        61623\\n20080314    -348        61275\\n20080317    14165       75440\\n20080318    -4111       71329\\n20080319    1421        72750\\n20080320    761     73511\\n20080325    387     73897\\n20080326    -1233       72664\\n20080327    280     72944\\n20080328    -1000       71944\\n20080331    -240        71704\\n20080401    2068        73772\\n20080402    -1139       72633\\n20080403    -2814       69818\\n20080404    0       69818\\n20080407    0       69818\\n20080408    -976        68842\\n20080409    -635        68207\\n20080410    2204        70412\\n20080411    -2373       68039\\n20080414    294     68333\\n20080415    1928        70261\\n20080416    1701        71962\\n20080417    -218        71744\\n20080418    5511        77255\\n20080421    397     77652\\n20080422    -694        76958\\n20080423    2855        79813\\n20080424    -109        79703\\n20080425    982     80685\\n20080428    959     81644\\n20080429    -1556       80088\\n20080430    2966        83053\\n20080502    869     83922\\n20080505    -598        83324\\n20080506    -2961       80363\\n20080507    1572        81935\\n20080508    80      82015\\n20080509    956     82971\\n20080512    64      83035\\n20080513    -351        82683\\n20080514    234     82917\\n20080515    -155        82763\\n20080516    -1792       80971\\n20080519    -1756       79215\\n20080520    -996        78219\\n20080521    -3039       75180\\n20080522    2047        77227\\n20080523    930     78156\\n20080526    -91     78065\\n20080527    1468        79534\\n20080528    855     80388\\n20080529    2082        82470\\n20080530    -1357       81114\\n20080602    766     81879\\n20080603    2640        84519\\n20080604    -1182       83337\\n20080605    -883        82454\\n20080606    284     82738\\n20080609    607     83345\\n20080610    -1099       82245\\n20080611    465     82710\\n20080612    2730        85440\\n20080613    3357        88797\\n20080616    858     89656\\n20080617    1707        91363\\n20080618    -1158       90205\\n20080619    -1267       88938\\n20080620    519     89457\\n20080623    -427        89029\\n20080624    -519        88511\\n20080625    488     88999\\n20080626    2398        91397\\n20080627    86      91483\\n20080630    2527        94010\\n20080701    981     94990\\n20080702    3712        98703\\n20080703    -2159       96544\\n20080704    -407        96137\\n20080707    -72     96065\\n20080708    -382        95684\\n20080709    3493        99176\\n20080710    646     99822\\n20080711    -1273       98549\\n20080714    1904        100453\\n20080715    219     100673\\n20080716    -131        100542\\n20080717    3144        103686\\n20080718    4944        108630\\n20080721    -1150       107480\\n20080722    -1189       106291\\n20080723    1711        108002\\n20080724    -1233       106769\\n20080725    1286        108055\\n20080728    -982        107073\\n20080729    749     107822\\n20080730    5316        113138\\n20080731    536     113674\\n20080801    -1509       112166\\n20080804    -462        111704\\n20080805    -1617       110087\\n20080806    -2006       108081\\n20080807    -396        107685\\n20080808    342     108027\\n20080811    1433        109460\\n20080812    -1532       107928\\n20080813    -932        106996\\n20080814    -756        106240\\n20080815    -1902       104338\\n20080818    -240        104098\\n20080819    -1652       102445\\n20080820    -247        102199\\n20080821    1876        104075\\n20080822    -122        103953\\n20080825    727     104679\\n20080826    424     105103\\n20080827    704     105807\\n20080828    -1731       104076\\n20080829    -1425       102651\\n20080901    386     103037\\n20080902    2854        105891\\n20080903    -554        105337\\n20080904    -1844       103493\\n20080905    -311        103182\\n20080908    -1237       101945\\n20080909    -901        101044\\n20080910    -230        100813\\n20080911    892     101706\\n20080912    -786        100919\\n20080915    60      100980\\n20080916    675     101654\\n20080917    274     101929\\n20080918    1770        103699\\n20080919    1341        105039\\n20080922    -3465       101574\\n20080923    1640        103214\\n20080924    670     103885\\n20080925    2537        106422\\n20080926    812     107234\\n20080929    682     107916\\n20080930    280     108196\\n20081001    4068        112264\\n20081002    2195        114459\\n20081003    1594        116053\\n20081006    4246        120298\\n20081007    2836        123134\\n20081008    4321        127455\\n20081009    -10120      117335\\n20081010    3316        120651\\n20081013    -1598       119053\\n20081014    347     119400\\n20081015    -8093       111308\\n20081016    0       111308\\n20081017    1124        112432\\n20081020    -955        111477\\n20081021    -4785       106692\\n20081022    -4139       102553\\n20081023    0       102553\\n20081024    3450        106003\\n20081027    654     106657\\n20081028    1201        107858\\n20081029    -5078       102780\\n20081030    -606        102175\\n20081031    1445        103619\\n20081103    -2367       101252\\n20081104    3176        104428\\n20081105    -2022       102406\\n20081106    -5305       97102\\n20081107    3269        100370\\n20081110    -1164       99206\\n20081111    7168        106375\\n20081112    -1697       104678\\n20081113    -5533       99145\\n20081114    3514        102658\\n20081117    -2201       100458\\n20081118    103     100561\\n20081119    1036        101597\\n20081120    -3323       98274\\n20081121    3779        102053\\n20081124    6253        108306\\n20081125    -310        107996\\n20081126    -2467       105528\\n20081127    5954        111482\\n20081128    -5365       106117\\n20081201    872     106989\\n20081202    2367        109356\\n20081203    4042        113399\\n20081204    1450        114848\\n20081205    -1379       113469\\n20081208    6825        120294\\n20081209    -88     120206\\n20081210    -55     120151\\n20081211    -4669       115482\\n20081212    1252        116734\\n20081215    1354        118088\\n20081216    -3373       114715\\n20081217    235     114950\\n20081218    -2468       112482\\n20081219    6824        119306\\n20081222    -2837       116469\\n20081223    -4      116465\\n20081229    -770        115695\\n20081230    392     116087\\n20090102    1867        117955\\n20090105    -4472       113483\\n20090106    -3416       110066\\n20090107    0       110066\\n20090108    0       110066\\n20090109    52      110119\\n20090112    -249        109870\\n20090113    2734        112604\\n20090114    8881        121485\\n20090115    -2455       119030\\n20090116    -198        118832\\n20090119    230     119062\\n20090120    1886        120948\\n20090121    -3702       117247\\n20090122    -1244       116003\\n20090123    -4808       111196\\n20090126    0       111196\\n20090127    0       111196\\n20090128    0       111196\\n20090129    -435        110761\\n20090130    -2336       108425\\n20090202    -275        108150\\n20090203    0       108150\\n20090204    -3830       104320\\n20090205    1278        105598\\n20090206    5668        111266\\n20090209    284     111550\\n20090210    -3889       107661\\n20090211    2495        110156\\n20090212    1108        111264\\n20090213    2685        113948\\n20090216    818     114766\\n20090217    -2319       112447\\n20090218    1189        113636\\n20090219    -2880       110756\\n20090220    1654        112410\\n20090223    733     113143\\n20090224    412     113555\\n20090225    -1263       112292\\n20090226    -4482       107810\\n20090227    -582        107229\\n20090302    2809        110037\\n20090303    -1284       108754\\n20090304    636     109390\\n20090305    -2882       106508\\n20090306    833     107341\\n20090309    -1663       105678\\n20090310    -6352       99326\\n20090311    1938        101264\\n20090312    1210        102474\\n20090313    -1935       100539\\n20090316    -688        99851\\n20090317    -2204       97647\\n20090318    1638        99284\\n20090319    2576        101861\\n20090320    375     102235\\n20090323    -2732       99503\\n20090324    1307        100810\\n20090325    6612        107422\\n20090326    -293        107129\\n20090327    -2379       104750\\n20090330    2742        107493\\n20090331    -1959       105533\\n20090401    946     106479\\n20090402    -2477       104001\\n20090403    7464        111465\\n20090406    -7020       104445\\n20090407    -3336       101109\\n20090408    2225        103334\\n20090409    -3459       99875\\n20090414    -713        99162\\n20090415    -71     99091\\n20090416    1230        100321\\n20090417    -631        99690\\n20090420    -2572       97118\\n20090421    727     97845\\n20090422    -4116       93729\\n20090423    6776        100505\\n20090424    3696        104201\\n20090427    985     105186\\n20090428    -4244       100942\\n20090429    6240        107182\\n20090430    -1630       105553\\n20090504    1060        106613\\n20090505    -2929       103684\\n20090506    -1419       102265\\n20090507    2562        104828\\n20090508    922     105750\\n20090511    1900        107651\\n20090512    595     108246\\n20090513    3797        112043\\n20090514    -2279       109763\\n20090515    523     110287\\n20090518    -2667       107620\\n20090519    4392        112012\\n20090520    -498        111514\\n20090521    -2519       108995\\n20090522    215     109210\\n20090525    -621        108589\\n20090526    126     108716\\n20090527    -858        107857\\n20090528    2036        109893\\n20090529    -1      109892\\n20090601    -3189       106703\\n20090602    -389        106314\\n20090603    219     106533\\n20090604    524     107057\\n20090605    2360        109417\\n20090608    -898        108520\\n20090609    -1937       106583\\n20090610    -2050       104533\\n20090611    493     105026\\n20090612    378     105405\\n20090615    2129        107534\\n20090616    -1088       106446\\n20090617    -11     106435\\n20090618    1360        107795\\n20090619    526     108321\\n20090622    1432        109753\\n20090623    2357        112110\\n20090624    -1823       110287\\n20090625    153     110441\\n20090626    -963        109478\\n20090629    950     110427\\n20090630    -1051       109376\\n20090701    149     109526\\n20090702    -3063       106463\\n20090703    -711        105752\\n20090706    -312        105440\\n20090707    -644        104796\\n20090708    96      104892\\n20090709    2619        107511\\n20090710    -1184       106326\\n20090713    2321        108647\\n20090714    270     108917\\n20090715    469     109386\\n20090716    389     109775\\n20090717    906     110681\\n20090720    -340        110341\\n20090721    2704        113045\\n20090722    -187        112858\\n20090723    2280        115138\\n20090724    391     115529\\n20090727    1436        116965\\n20090728    -647        116318\\n20090729    2547        118865\\n20090730    -1457       117408\\n20090731    -2441       114968\\n20090803    -1691       113276\\n20090804    396     113672\\n20090805    -1325       112347\\n20090806    -2574       109774\\n20090807    1007        110781\\n20090810    -398        110383\\n20090811    -1113       109270\\n20090812    2381        111651\\n20090813    -178        111473\\n20090814    341     111814\\n20090817    263     112076\\n20090818    981     113058\\n20090819    1434        114492\\n20090820    1180        115672\\n20090821    1035        116707\\n20090824    358     117065\\n20090825    -3168       113897\\n20090826    -1113       112784\\n20090827    -1419       111364\\n20090828    1324        112689\\n20090831    -217        112472\\n20090901    -609        111863\\n20090902    1474        113336\\n20090903    1028        114365\\n20090904    936     115301\\n20090907    1900        117200\\n20090908    1118        118318\\n20090909    1202        119520\\n20090910    2764        122285\\n20090911    -1282       121002\\n20090914    1847        122849\\n20090915    1017        123866\\n20090916    935     124801\\n20090917    -1012       123789\\n20090918    602     124391\\n20090921    -625        123766\\n20090922    -547        123219\\n20090923    -1164       122055\\n20090924    1571        123627\\n20090925    1393        125019\\n20090928    436     125455\\n20090929    -166        125289\\n20090930    2139        127427\\n20091001    1289        128716\\n20091002    1388        130104\\n20091005    -105        129999\\n20091006    -2551       127448\\n20091007    -190        127258\\n20091008    2361        129618\\n20091009    791     130409\\n20091012    2661        133070\\n20091013    -146        132924\\n20091014    5591        138515\\n20091015    1055        139570\\n20091016    -2741       136829\\n20091019    1395        138224\\n20091020    -1881       136343\\n20091021    440     136782\\n20091022    1475        138258\\n20091023    -231        138027\\n20091026    1162        139190\\n20091027    -969        138221\\n20091028    3494        141715\\n20091029    -2884       138831\\n20091030    2642        141472\\n20091102    -191        141281\\n20091103    401     141682\\n20091104    -723        140960\\n20091105    1558        142517\\n20091106    -613        141904\\n20091109    828     142733\\n20091110    -668        142065\\n20091111    -850        141215\\n20091112    -154        141061\\n20091113    249     141310\\n20091116    -1774       139536\\n20091117    -924        138611\\n20091118    1252        139863\\n20091119    -864        138999\\n20091120    -1327       137672\\n20091123    -2055       135617\\n20091124    461     136078\\n20091125    -1574       134504\\n20091126    -835        133669\\n20091127    316     133985\\n20091130    837     134822\\n20091201    1321        136143\\n20091202    -861        135282\\n20091203    -5660       129622\\n20091204    2010        131632\\n20091207    912     132544\\n20091208    -8      132536\\n20091209    2405        134941\\n20091210    -852        134089\\n20091211    811     134900\\n20091214    1523        136423\\n20091215    616     137039\\n20091216    232     137271\\n20091217    -808        136463\\n20091218    -251        136212\\n20091221    811     137023\\n20091222    -535        136488\\n20091223    -371        136118\\n20091228    1248        137366\\n20091229    -892        136474\\n20091230    -55     136419\\n20100104    510     136928\\n20100105    737     137666\\n20100106    587     138253\\n20100107    1705        139958\\n20100108    832     140791\\n20100111    -948        139843\\n20100112    -256        139586\\n20100113    1200        140786\\n20100114    -615        140171\\n20100115    -405        139766\\n20100118    1101        140867\\n20100119    281     141148\\n20100120    -945        140204\\n20100121    -1979       138225\\n20100122    -462        137763\\n20100125    -1280       136483\\n20100126    -5185       131297\\n20100127    -884        130414\\n20100128    -365        130049\\n20100129    -369        129679\\n20100201    -1092       128588\\n20100202    -430        128158\\n20100203    -508        127650\\n20100204    1628        129278\\n20100205    921     130199\\n20100208    62      130262\\n20100209    -922        129339\\n20100210    1015        130354\\n20100211    -1000       129355\\n20100212    2291        131646\\n20100215    -1138       130508\\n20100216    62      130570\\n20100217    -1475       129095\\n20100218    594     129689\\n20100219    729     130418\\n20100222    -343        130075\\n20100223    -101        129974\\n20100224    854     130827\\n20100225    -369        130458\\n20100226    129     130587\\n20100301    1762        132349\\n20100302    1479        133827\\n20100303    1727        135555\\n20100304    -578        134977\\n20100305    1447        136423\\n20100308    -1551       134873\\n20100309    1397        136270\\n20100310    -1056       135214\\n20100311    850     136064\\n20100312    -2546       133518\\n20100315    1092        134610\\n20100316    -2574       132036\\n20100317    2811        134847\\n20100318    -60     134788\\n20100319    -37     134751\\n20100322    20      134771\\n20100323    460     135231\\n20100324    216     135448\\n20100325    1619        137067\\n20100326    -179        136887\\n20100329    830     137717\\n20100330    -3199       134518\\n20100331    -1799       132719\\n20100401    -1142       131576\\n20100406    -620        130956\\n20100407    0       130956\\n20100408    0       130956\\n20100409    1602        132558\\n20100412    353     132912\\n20100413    594     133505\\n20100414    577     134083\\n20100415    553     134635\\n20100416    1461        136097\\n20100419    -1548       134548\\n20100420    -1708       132840\\n20100421    -71     132770\\n20100422    -1766       131004\\n20100423    7156        138160\\n20100426    4505        142666\\n20100427    -1854       140812\\n20100428    -1227       139585\\n20100429    1109        140694\\n20100430    670     141364\\n20100503    -2792       138572\\n20100504    1889        140461\\n20100505    -835        139626\\n20100506    -1636       137990\\n20100507    128     138118\\n20100510    -4930       133188\\n20100511    1741        134929\\n20100512    -803        134126\\n20100513    2311        136437\\n20100514    -2623       133814\\n20100517    570     134384\\n20100518    1692        136076\\n20100519    -1094       134982\\n20100520    -2300       132682\\n20100521    -569        132113\\n20100524    48      132160\\n20100525    1525        133685\\n20100526    -2577       131108\\n20100527    -986        130123\\n20100528    -702        129420\\n20100531    1495        130915\\n20100601    -493        130422\\n20100602    -91     130331\\n20100603    494     130825\\n20100604    -1586       129239\\n20100607    458     129697\\n20100608    -3701       125996\\n20100609    2625        128621\\n20100610    1732        130353\\n20100611    -1752       128601\\n20100614    2890        131491\\n20100615    364     131855\\n20100616    248     132103\\n20100617    -465        131638\\n20100618    -645        130994\\n20100621    -1323       129670\\n20100622    -206        129464\\n20100623    170     129633\\n20100624    1119        130753\\n20100625    -404        130348\\n20100628    -1908       128440\\n20100629    736     129176\\n20100630    92      129268\\n20100701    885     130153\\n20100702    1443        131596\\n20100705    91      131687\\n20100706    1182        132869\\n20100707    -2      132867\\n20100708    -791        132076\\n20100709    328     132404\\n20100712    -1151       131253\\n20100713    1478        132731\\n20100714    1180        133911\\n20100715    -931        132980\\n20100716    3147        136127\\n20100719    -1421       134706\\n20100720    -823        133883\\n20100721    480     134363\\n20100722    1398        135761\\n20100723    1189        136951\\n20100726    163     137113\\n20100727    -1420       135693\\n20100728    1186        136879\\n20100729    1994        138873\\n20100730    -1087       137786\\n20100802    -821        136966\\n20100803    -166        136799\\n20100804    1899        138699\\n20100805    -2154       136545\\n20100806    -1178       135366\\n20100809    1397        136763\\n20100810    -430        136334\\n20100811    -229        136104\\n20100812    670     136774\\n20100813    530     137304\\n20100816    624     137929\\n20100817    -832        137096\\n20100818    170     137266\\n20100819    -2130       135136\\n20100820    215     135351\\n20100823    539     135890\\n20100824    1099        136989\\n20100825    -427        136562\\n20100826    -1033       135529\\n20100827    478     136007\\n20100830    -593        135414\\n20100831    1338        136752\\n20100901    1784        138535\\n20100902    -874        137661\\n20100903    104     137766\\n20100906    -1673       136092\\n20100907    -330        135763\\n20100908    -377        135385\\n20100909    -1388       133998\\n20100910    645     134643\\n20100913    41      134684\\n20100914    2123        136807\\n20100915    -289        136518\\n20100916    3515        140033\\n20100917    970     141004\\n20100920    2108        143111\\n20100921    410     143521\\n20100922    -1910       141611\\n20100923    -193        141418\\n20100924    -3463       137955\\n20100927    0       137955\\n20100928    0       137955\\n20100929    0       137955\\n20100930    0       137955\\n20101001    0       137955\\n20101004    876     138831\\n20101005    -957        137874\\n20101006    -2043       135831\\n20101007    1400        137231\\n20101008    -1164       136067\\n20101011    89      136156\\n20101012    -2067       134089\\n20101013    2048        136136\\n20101014    -19     136117\\n20101015    -494        135623\\n20101018    -312        135311\\n20101019    199     135510\\n20101020    -1424       134086\\n20101021    -579        133507\\n20101022    -27     133480\\n20101025    428     133908\\n20101026    1231        135138\\n20101027    1766        136904\\n20101028    -387        136518\\n20101029    1000        137518\\n20101101    -716        136802\\n20101102    -73     136729\\n20101103    298     137027\\n20101104    -1138       135889\\n20101105    990     136879\\n20101108    2738        139617\\n20101109    206     139823\\n20101110    -3267       136556\\n20101111    4067        140623\\n20101112    1470        142093\\n20101115    -513        141580\\n20101116    -1654       139926\\n20101117    -521        139405\\n20101118    -984        138421\\n20101119    -252        138169\\n20101122    -374        137796\\n20101123    353     138149\\n20101124    -1781       136368\\n20101125    846     137213\\n20101126    -427        136787\\n20101129    77      136864\\n20101130    -3      136861\\n20101201    2978        139839\\n20101202    2570        142409\\n20101203    -437        141972\\n20101206    985     142958\\n20101207    1558        144515\\n20101208    -857        143658\\n20101209    -3368       140291\\n20101210    -837        139453\\n20101213    182     139636\\n20101214    333     139968\\n20101215    -3387       136581\\n20101216    0       136581\\n20101217    0       136581\\n20101220    1239        137819\\n20101221    -373        137446\\n20101222    -713        136733\\n20101223    803     137535\\n20101227    -16     137520\\n20101228    -765        136755\\n20101229    65      136820\\n20101230    -1688       135132\\n20110103    -336        134796\\n20110104    3070        137866\\n20110105    2524        140390\\n20110106    730     141119\\n20110107    -631        140489\\n20110110    -391        140098\\n20110111    -2308       137790\\n20110112    -863        136927\\n20110113    -3233       133694\\n20110114    0       133694\\n20110117    -1164       132530\\n20110118    1579        134109\\n20110119    -961        133148\\n20110120    4029        137177\\n20110121    -1153       136024\\n20110124    -2623       133401\\n20110125    -1764       131637\\n20110126    1884        133521\\n20110127    684     134204\\n20110128    -475        133730\\n20110131    -115        133615\\n20110201    -160        133455\\n20110202    -1759       131696\\n20110203    146     131842\\n20110204    -121        131721\\n20110207    -867        130854\\n20110208    -620        130235\\n20110209    333     130567\\n20110210    793     131361\\n20110211    -1102       130259\\n20110214    -948        129311\\n20110215    367     129678\\n20110216    3691        133369\\n20110217    91      133459\\n20110218    107     133567\\n20110221    -679        132887\\n20110222    -1806       131081\\n20110223    2410        133491\\n20110224    -6532       126959\\n20110225    0       126959\\n20110228    4272        131231\\n20110301    1       131232\\n20110302    -565        130667\\n20110303    694     131361\\n20110304    -222        131139\\n20110307    -474        130665\\n20110308    -654        130011\\n20110309    -141        129870\\n20110310    1840        131710\\n20110311    1805        133515\\n20110314    -3264       130251\\n20110315    -2615       127636\\n20110316    -350        127287\\n20110317    2185        129472\\n20110318    804     130276\\n20110321    2549        132825\\n20110322    -1826       130999\\n20110323    -308        130691\\n20110324    560     131250\\n20110325    271     131521\\n20110328    347     131869\\n20110329    489     132358\\n20110330    607     132965\\n20110331    1108        134072\\n20110401    1360        135432\\n20110404    -2800       132632\\n20110405    1433        134065\\n20110406    -425        133640\\n20110407    1696        135336\\n20110408    939     136275\\n20110411    1156        137431\\n20110412    1427        138858\\n20110413    132     138990\\n20110414    -530        138460\\n20110415    -2522       135938\\n20110418    -871        135067\\n20110419    1020        136086\\n20110420    556     136643\\n20110421    7651        144293\\n20110426    280     144573\\n20110427    489     145062\\n20110428    -440        144623\\n20110429    -335        144288\\n20110502    -553        143735\\n20110503    189     143924\\n20110504    1811        145735\\n20110505    -275        145461\\n20110506    -711        144750\\n20110509    -314        144436\\n20110510    -6      144430\\n20110511    528     144958\\n20110512    -101        144857\\n20110513    894     145751\\n20110516    -1692       144059\\n20110517    -730        143329\\n20110518    -245        143085\\n20110519    2370        145455\\n20110520    -1161       144294\\n20110523    400     144693\\n20110524    339     145032\\n20110525    -2681       142351\\n20110526    -459        141893\\n20110527    -573        141319\\n20110530    -2144       139175\\n20110531    2341        141516\\n20110601    -739        140777\\n20110602    432     141210\\n20110603    3898        145108\\n20110606    -105        145003\\n20110607    -1912       143091\\n20110608    -755        142335\\n20110609    -890        141446\\n20110610    -356        141090\\n20110613    54      141143\\n20110614    -190        140953\\n20110615    -2034       138919\\n20110616    1422        140341\\n20110617    -181        140160\\n20110620    1160        141320\\n20110621    48      141368\\n20110622    -1410       139959\\n20110623    -1553       138406\\n20110624    -911        137495\\n20110627    -829        136666\\n20110628    -2144       134523\\n20110629    -1108       133415\\n20110630    6       133421\\n20110701    1355        134776\\n20110704    1399        136176\\n</pre>\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example of large numerical table\n",
    "#I wonder if i should filter out rows with `<pre>`?\n",
    "df.loc[99414].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(txt):\n",
    "    txt = re.sub(r'<pre>.*?</pre>', r'', txt, flags=re.S)\n",
    "    txt = re.sub(r'<[^<]+?>', '', txt) #html tags\n",
    "    txt = re.sub(r'\\$[^$]+\\$', '', txt)  #latex\n",
    "    txt = re.sub(r'https?://[^\\s]*', '', txt) #remove URLs\n",
    "    txt = re.sub(r'\\s+', ' ', txt) #condense spaces \n",
    "    return txt\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I’m working on a trading system and need to apply some statistics on the results. Unfortunately I forgot all about statistics after I left university over a decade ago and now I really have no clue how I must calculate what I need. Hopefully someone can help me out. Out of the trading application (currently in test mode), I get profit / loss (PL) per trade and per day. Let’s say I have the day-to-day PL (an accumulation will give the total PL over the given period) of 5 years back testing (about 1250 points), what is the best way of “predicting” what the total profit might be in the next 6 months (125 points ahead) and the next year (250 points ahead)? Of course not every trade is profitable. So I have some trades with losses and (hopefully) more trades with profit. What is the best way of calculating what the profit per day (with a certain reliability) will be when you only take the winning trades into account, what lose will be when you only look at the losing trades and what the PL will be when you take both winning and losing trade combined. How can I calculate the change that over an X period of time, the strategy will, with a certain percentage of reliability) make money? Furthermore, I will of course not trade just one strategy, but multiple. Based on back testing, how can I calculate the likelihood that the portfolio I will be trading will make money in x period and how can I calculate the bandwidth for the expected profit for the pool? Hope you can help me out which methods to use (I’ll be programming it in C# by the way). Example of day-to-day PL (rounded rounded to 0 decimals for the layout): '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[99414].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id  parent_id  comment_id  \\\n",
       "0        1        NaN         NaN   \n",
       "1        2        NaN         NaN   \n",
       "2        3        NaN         NaN   \n",
       "3        4        NaN         NaN   \n",
       "4        6        NaN         NaN   \n",
       "\n",
       "                                                text category  \n",
       "0                      Eliciting priors from experts    title  \n",
       "1                                 What is normality?    title  \n",
       "2  What are some valuable Statistical Analysis op...    title  \n",
       "3  Assessing the significance of differences in d...    title  \n",
       "4  The Two Cultures: statistics vs. machine learn...    title  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
